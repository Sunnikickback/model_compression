{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-05T01:19:19.829797Z",
     "start_time": "2024-04-05T01:19:19.722978Z"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from transformers import RobertaForSequenceClassification, RobertaConfig, RobertaTokenizer\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler, TensorDataset\n",
    "from model_compression.training_utils.datasets import (processors, load_and_cache_examples)\n",
    "from model_compression.training_utils.modules import RobertaForSpanClassification\n",
    "from model_compression.training_utils.metrics import superglue_compute_metrics\n",
    "from model_compression.training_utils.training_utils import train, evaluate\n",
    "from model_compression.training_utils.utils import TrainConfig, output_modes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7badd025",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "775c51baa88d5dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_CLASSES = {\n",
    "    \"roberta\": (\n",
    "        RobertaConfig,\n",
    "        RobertaTokenizer,\n",
    "        {\"classification\": RobertaForSequenceClassification, \"span_classification\": RobertaForSpanClassification},\n",
    "    ),\n",
    "}\n",
    "tasks_num_spans = {\n",
    "    \"wic\": 2,\n",
    "    \"wsc\": 2,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b26ded4a93cc0ea",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-05T01:19:28.918613Z",
     "start_time": "2024-04-05T01:19:28.910867Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5fba2326",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Optional, Tuple, Union\n",
    "@dataclass\n",
    "class TrainConfig:\n",
    "    weight_decay: float = 0.01\n",
    "    max_steps: int = -1\n",
    "    gradient_accumulation_steps: int = 1\n",
    "    num_train_epochs: int = 30\n",
    "    warmup_ratio: float = 0.06\n",
    "    learning_rate: float = 0.00001\n",
    "    adam_epsilon:float = 1e-8\n",
    "    max_grad_norm:float = 1.0\n",
    "    train_batch_size: int = 16 \n",
    "    eval_batch_size:int = 32\n",
    "    eval_and_save_steps:float = 500\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "538701e721e53789",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-04T09:25:06.093262Z",
     "start_time": "2024-04-04T09:25:06.087498Z"
    }
   },
   "outputs": [],
   "source": [
    "task_name = \"BoolQ\"\n",
    "model_type = \"roberta\"\n",
    "model_checkpoint = \"roberta-base\"\n",
    "tokenizer_name = model_checkpoint\n",
    "do_lower_case = False\n",
    "data_dir = \"../../data/BoolQ/\"\n",
    "output_dir = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8ea9e4575c16739d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-04T09:25:06.096784Z",
     "start_time": "2024-04-04T09:25:06.093257Z"
    }
   },
   "outputs": [],
   "source": [
    "# Prepare task\n",
    "task_name = task_name.lower()\n",
    "assert task_name in processors, f\"Task {task_name} not found!\"\n",
    "processor = processors[task_name]()\n",
    "output_mode = output_modes[task_name]\n",
    "label_list = processor.get_labels() \n",
    "num_labels = len(label_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c197c3610145c079",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-04T09:25:06.100924Z",
     "start_time": "2024-04-04T09:25:06.096151Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RobertaForSequenceClassification(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): RobertaClassificationHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (out_proj): Linear(in_features=768, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_type = model_type.lower()\n",
    "config_class, tokenizer_class, model_classes = MODEL_CLASSES[model_type]\n",
    "model_class = model_classes[output_mode]\n",
    "config = config_class.from_pretrained(\n",
    "    model_checkpoint,\n",
    "    num_labels=num_labels,\n",
    "    finetuning_task=task_name,\n",
    ")\n",
    "if output_mode == \"span_classification\":\n",
    "    config.num_spans = tasks_num_spans[task_name]\n",
    "tokenizer = tokenizer_class.from_pretrained(\n",
    "    tokenizer_name,\n",
    "    do_lower_case=do_lower_case,\n",
    ")\n",
    "model = model_class.from_pretrained(\n",
    "            model_checkpoint,\n",
    "            config=config,\n",
    "        )\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f936be6214980df5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-04T09:25:06.101182Z",
     "start_time": "2024-04-04T09:25:06.098795Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    }
   ],
   "source": [
    "train_dataset = load_and_cache_examples(task_name, tokenizer, data_dir, max_seq_length=512) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "587c9259",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_config = TrainConfig(train_batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee7630b41ba2ffbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:457: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Epoch 0 loss: 0.634: 100%|████████████████████| 295/295 [01:35<00:00,  3.07it/s]\n",
      "Epoch 1 loss: 0.553:  69%|█████████████▊      | 204/295 [01:04<00:28,  3.17it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    }
   ],
   "source": [
    "global_step, tr_loss = train(train_dataset, model, tokenizer, \n",
    "                             output_mode = output_mode,\n",
    "                             model_type=model_type,\n",
    "                             train_config=train_config, task_name=task_name, data_dir=data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec6b7aa9be50ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "result, preds, ex_ids = evaluate(task_name, model, tokenizer, eval_batch_size=train_config.eval_batch_size,\n",
    "                                             device=\"cuda:0\", use_fixed_seq_length=False, output_mode=output_mode,\n",
    "                                             model_type=model_type, use_tqdm=True, data_dir=data_dir)\n",
    "result = dict((f\"{k}\", v) for k, v in result.items())\n",
    "\n",
    "eval_task_names = (task_name,) \n",
    "\n",
    "for eval_task_name in eval_task_names:\n",
    "    result, preds, ex_ids = evaluate(eval_task_name, model, tokenizer, eval_batch_size=train_config.eval_batch_size,\n",
    "                                     device=\"cuda:0\", use_fixed_seq_length=False, output_mode=output_mode,\n",
    "                                     model_type=model_type, use_tqdm=True, data_dir=data_dir,split=\"test\", prefix=\"\")\n",
    "    \n",
    "    processor = processors[eval_task_name]()\n",
    "    if task_name == \"record\":\n",
    "        answers = processor.get_answers(data_dir, \"test\")\n",
    "        processor.write_preds(preds, ex_ids, output_dir, answers=answers)\n",
    "    else:\n",
    "        processor.write_preds(preds, ex_ids, output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afaa431e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8862393c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae9c1c0f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc8cc5f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
